{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SeongBeomLEE/RecsysTutorial/blob/main/CDAE/CDAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해당 모델에 bpr loss 적용될 수 없을지 생각해보기 -> objective function으로 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ug-br-pPu9vZ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from box import Box\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "torch.set_printoptions(sci_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbRKDSg4u9vc"
   },
   "source": [
    "# 1. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mlm1UrKvoC_O"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "'data_path' : \"/opt/ml/input/data/train\" , # 데이터 경로\n",
    "'save_dir': './checkpoint/',\n",
    "'saved_file_name':'best_model.pt',\n",
    "'p_dims': [50, 700],\n",
    "'dropout_rate' : 0.8,\n",
    "'weight_decay' : 0.01,\n",
    "'valid_samples' : 10, # 검증에 사용할 sample 수\n",
    "'patience' : 30,\n",
    "'lr' : 0.001,\n",
    "'batch_size' : 500,\n",
    "'num_epochs' : 30, # Recommendation : 200\n",
    "'num_workers' : 2,\n",
    "}\n",
    "config = Box(config)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjDxy0fJu9vf"
   },
   "source": [
    "# 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "W64BYWl0u9vg"
   },
   "outputs": [],
   "source": [
    "class MakeMatrixDataSet():\n",
    "    \"\"\"\n",
    "    MatrixDataSet 생성\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'train_ratings.csv'))\n",
    "        \n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('item')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('user')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['item'].apply(lambda x : self.item_encoder[x])\n",
    "        self.df['user_idx'] = self.df['user'].apply(lambda x : self.user_encoder[x])\n",
    "\n",
    "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
    "\n",
    "    def generate_encoder_decoder(self, col : str) -> dict:\n",
    "        \"\"\"\n",
    "        encoder, decoder 생성\n",
    "\n",
    "        Args:\n",
    "            col (str): 생성할 columns 명\n",
    "        Returns:\n",
    "            dict: 생성된 user encoder, decoder\n",
    "        \"\"\"\n",
    "\n",
    "        encoder = {}\n",
    "        decoder = {}\n",
    "        ids = self.df[col].unique()\n",
    "\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "\n",
    "        return encoder, decoder\n",
    "    \n",
    "    def generate_sequence_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        sequence_data 생성\n",
    "\n",
    "        Returns:\n",
    "            dict: train user sequence / valid user sequence\n",
    "        \"\"\"\n",
    "        users = defaultdict(list)\n",
    "        user_train = {}\n",
    "        user_valid = {}\n",
    "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['time']):\n",
    "            users[user].append(item)\n",
    "        \n",
    "        for user in users:\n",
    "            user_total = users[user]\n",
    "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
    "            train = list(set(user_total) - set(valid))\n",
    "\n",
    "            user_train[user] = train\n",
    "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
    "\n",
    "        return user_train, user_valid\n",
    "    \n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.user_valid\n",
    "\n",
    "    def make_matrix(self, user_list, train = True):\n",
    "        \"\"\"\n",
    "        user_item_dict를 바탕으로 행렬 생성\n",
    "        \"\"\"\n",
    "        mat = torch.zeros(size = (user_list.size(0), self.num_item))\n",
    "        for idx, user in enumerate(user_list):\n",
    "            if train:\n",
    "                mat[idx, self.user_train[user.item()]] = 1\n",
    "            else:\n",
    "                mat[idx, self.user_train[user.item()] + self.user_valid[user.item()]] = 1\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IldCGmY8u9vh"
   },
   "outputs": [],
   "source": [
    "class AEDataSet(Dataset):\n",
    "    def __init__(self, num_user):\n",
    "        self.num_user = num_user\n",
    "        self.users = [i for i in range(num_user)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_user\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        user = self.users[idx]\n",
    "        return torch.LongTensor([user])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jtf1I824nx5V"
   },
   "source": [
    "# 3. 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Q4MKBcN1DM3K"
   },
   "outputs": [],
   "source": [
    "class CDAE(nn.Module):\n",
    "\n",
    "    def __init__(self, p_dims, user_num, item_num, dropout_rate = 0.5):\n",
    "        super(CDAE, self).__init__()\n",
    "        self.p_dims = p_dims\n",
    "        self.q_dims = p_dims[::-1]\n",
    "\n",
    "        self.dims = self.q_dims + self.p_dims[1:]\n",
    "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(self.dims[:-1], self.dims[1:])])\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        self.user_embedding = nn.Embedding(user_num, item_num)\n",
    "        self.init_weights()\n",
    "        \n",
    "    \n",
    "    def forward(self, _input, user):\n",
    "        h = F.normalize(_input)\n",
    "        h = self.drop(h)\n",
    "        h += self.user_embedding(user)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.layers) - 1:\n",
    "                h = F.sigmoid(h)\n",
    "        return h\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VtqiAf62DV2d"
   },
   "outputs": [],
   "source": [
    "class LossFunc(nn.Module):\n",
    "\n",
    "    def __init__(self, loss_type = 'Multinomial', model_type = None):\n",
    "        super(LossFunc, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def forward(self, recon_x = None, x = None, mu = None, logvar = None, anneal = None):\n",
    "        if self.loss_type == 'Gaussian':\n",
    "            loss = self.Gaussian(recon_x, x)\n",
    "        elif self.loss_type == 'Logistic':\n",
    "            loss = self.Logistic(recon_x, x)\n",
    "        elif self.loss_type == 'Multinomial':\n",
    "            loss = self.Multinomial(recon_x, x)\n",
    "\n",
    "        \n",
    "        if self.model_type == 'VAE':\n",
    "            KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "            loss = loss + anneal * KLD\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def Gaussian(self, recon_x, x):\n",
    "        gaussian = F.mse_loss(recon_x, x)\n",
    "        return gaussian\n",
    "\n",
    "    def Logistic(self, recon_x, x):\n",
    "        logistic = F.binary_cross_entropy(recon_x.sigmoid(), x, reduction='none').sum(1).mean()\n",
    "        return logistic\n",
    "\n",
    "    def Multinomial(self, recon_x, x):\n",
    "        multinomial = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "        return multinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dk-bL5p4nx5W"
   },
   "source": [
    "# 4. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hmJ40xRDDjcX"
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, data_loader, make_matrix_data_set):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for users in data_loader:\n",
    "        mat = make_matrix_data_set.make_matrix(users)\n",
    "        mat = mat.to(device)\n",
    "        recon_mat = model(mat, users.view(-1).to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(recon_x = recon_mat, x = mat)\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def get_ndcg(pred_list, true_list):\n",
    "    idcg = sum((1 / np.log2(rank + 2) for rank in range(1, len(pred_list))))\n",
    "    dcg = 0\n",
    "    for rank, pred in enumerate(pred_list):\n",
    "        if pred in true_list:\n",
    "            dcg += 1 / np.log2(rank + 2)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "# hit == recall == precision\n",
    "def get_hit(pred_list, true_list):\n",
    "    hit_list = set(true_list) & set(pred_list)\n",
    "    hit = len(hit_list) / len(true_list)\n",
    "    return hit\n",
    "\n",
    "def evaluate(model, data_loader, user_train, user_valid, make_matrix_data_set):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    NDCG = 0.0 # NDCG@10\n",
    "    HIT = 0.0 # HIT@10\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users in data_loader:\n",
    "            mat = make_matrix_data_set.make_matrix(users)\n",
    "            mat = mat.to(device)\n",
    "\n",
    "            recon_mat = model(mat, users.view(-1).to(device))\n",
    "            recon_mat[mat == 1] = -np.inf\n",
    "            rec_list = recon_mat.argsort(dim = 1)\n",
    "\n",
    "            for user, rec in zip(users, rec_list):\n",
    "                uv = user_valid[user.item()]\n",
    "                up = rec[-10:].cpu().numpy().tolist()[::-1]\n",
    "                NDCG += get_ndcg(pred_list = up, true_list = uv)\n",
    "                HIT += get_hit(pred_list = up, true_list = uv)\n",
    "\n",
    "    NDCG /= len(data_loader.dataset)\n",
    "    HIT /= len(data_loader.dataset)\n",
    "\n",
    "    return NDCG, HIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, data_loader, decoder, make_matrix_data_set):  \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    user_list,item_list,prob_list = [],[],[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users in data_loader:\n",
    "            mat = make_matrix_data_set.make_matrix(users)\n",
    "            mat = mat.to(device)\n",
    "\n",
    "            recon_mat = model(mat, users.view(-1).to(device))\n",
    "            recon_mat[mat == 1] = -np.inf\n",
    "            rec_list = recon_mat.argsort(dim = 1)\n",
    "            rec_prob = recon_mat.sort(dim = 1)\n",
    "\n",
    "            for user, rec in zip(users, rec_list):\n",
    "                rec_items = rec[-10:].cpu().numpy().tolist()[::-1]\n",
    "                rec_items = [decoder['item'][int(item)]for item in rec_items]\n",
    "                rec_users = decoder['user'][int(user)].repeat(10)\n",
    "                \n",
    "                user_list = np.concatenate([user_list,rec_users])\n",
    "                item_list = np.concatenate([item_list,rec_items])\n",
    "                \n",
    "\n",
    "    submit = pd.DataFrame(zip(user_list,item_list), columns=['user','item']).astype({'user':'int','item':'int'})\n",
    "    submit = submit.sort_values(by=['user'])\n",
    "    return submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozBvgarCnx5W"
   },
   "source": [
    "# 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Cnq1T8S6nx5X"
   },
   "outputs": [],
   "source": [
    "make_matrix_data_set = MakeMatrixDataSet(config = config)\n",
    "user_train, user_valid = make_matrix_data_set.get_train_valid_data()\n",
    "decoder = {}\n",
    "_,decoder['user'] = make_matrix_data_set.generate_encoder_decoder('user')\n",
    "_,decoder['item'] = make_matrix_data_set.generate_encoder_decoder('item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "RKHjKcKinx5X"
   },
   "outputs": [],
   "source": [
    "ae_dataset = AEDataSet(\n",
    "    num_user = make_matrix_data_set.num_user,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "r0k5yCsSnx5X"
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    ae_dataset,\n",
    "    batch_size = config.batch_size, \n",
    "    shuffle = True, \n",
    "    pin_memory = True,\n",
    "    num_workers = config.num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "o2sFDKhFnx5X"
   },
   "outputs": [],
   "source": [
    "model = CDAE(\n",
    "    p_dims = config.p_dims + [make_matrix_data_set.num_item],\n",
    "    user_num = make_matrix_data_set.num_user, \n",
    "    item_num = make_matrix_data_set.num_item,\n",
    "    dropout_rate = config.dropout_rate).to(device)\n",
    "criterion = LossFunc(loss_type = 'Logistic')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RaWc3b1nx5X",
    "outputId": "dd13a4aa-c9d0-4fe0-8f30-c06870a5334c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Train loss: 815.81133| NDCG@10: 0.13146| HIT@10: 0.08847\n",
      "Epoch:   2| Train loss: 561.60996| NDCG@10: 0.13164| HIT@10: 0.08863\n",
      "Epoch:   3| Train loss: 558.46746| NDCG@10: 0.13096| HIT@10: 0.08859\n",
      "Epoch:   4| Train loss: 537.65672| NDCG@10: 0.13032| HIT@10: 0.08868\n",
      "Epoch:   5| Train loss: 521.38920| NDCG@10: 0.13020| HIT@10: 0.08855\n",
      "Epoch:   6| Train loss: 514.09129| NDCG@10: 0.13027| HIT@10: 0.08822\n",
      "Epoch:   7| Train loss: 508.22806| NDCG@10: 0.13073| HIT@10: 0.08822\n",
      "Epoch:   8| Train loss: 504.78173| NDCG@10: 0.13036| HIT@10: 0.08880\n",
      "Epoch:   9| Train loss: 502.18185| NDCG@10: 0.13107| HIT@10: 0.08845\n",
      "Epoch:  10| Train loss: 500.81223| NDCG@10: 0.12973| HIT@10: 0.08750\n",
      "Epoch:  11| Train loss: 499.81687| NDCG@10: 0.13104| HIT@10: 0.08835\n",
      "Epoch:  12| Train loss: 498.95233| NDCG@10: 0.13106| HIT@10: 0.08841\n",
      "Epoch:  13| Train loss: 497.93820| NDCG@10: 0.12975| HIT@10: 0.08871\n",
      "Epoch:  14| Train loss: 495.45230| NDCG@10: 0.13178| HIT@10: 0.08930\n",
      "Epoch:  15| Train loss: 492.06213| NDCG@10: 0.13309| HIT@10: 0.08999\n",
      "Epoch:  16| Train loss: 489.18700| NDCG@10: 0.13605| HIT@10: 0.09160\n",
      "Epoch:  17| Train loss: 486.82505| NDCG@10: 0.13556| HIT@10: 0.09137\n",
      "Epoch:  18| Train loss: 484.74701| NDCG@10: 0.13827| HIT@10: 0.09298\n",
      "Epoch:  19| Train loss: 483.36691| NDCG@10: 0.13819| HIT@10: 0.09325\n",
      "Epoch:  20| Train loss: 482.03336| NDCG@10: 0.13642| HIT@10: 0.09303\n",
      "Epoch:  21| Train loss: 481.22114| NDCG@10: 0.13907| HIT@10: 0.09472\n",
      "Epoch:  22| Train loss: 480.30106| NDCG@10: 0.14090| HIT@10: 0.09504\n",
      "Epoch:  23| Train loss: 479.39675| NDCG@10: 0.13967| HIT@10: 0.09533\n",
      "Epoch:  24| Train loss: 478.36853| NDCG@10: 0.14030| HIT@10: 0.09570\n",
      "Epoch:  25| Train loss: 477.62093| NDCG@10: 0.14176| HIT@10: 0.09635\n",
      "Epoch:  26| Train loss: 476.51726| NDCG@10: 0.14148| HIT@10: 0.09651\n",
      "Epoch:  27| Train loss: 475.04196| NDCG@10: 0.14502| HIT@10: 0.09828\n",
      "Epoch:  28| Train loss: 472.47228| NDCG@10: 0.14725| HIT@10: 0.10018\n",
      "Epoch:  29| Train loss: 468.89231| NDCG@10: 0.15245| HIT@10: 0.10308\n",
      "Epoch:  30| Train loss: 465.54180| NDCG@10: 0.15494| HIT@10: 0.10523\n",
      "Best NDCG@10: 0.15494| Best HIT@10: 0.10523\n"
     ]
    }
   ],
   "source": [
    "def main(config):\n",
    "    best_hit,best_ndcg = -1,-1\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        train_loss = train(\n",
    "            model = model, \n",
    "            criterion = criterion, \n",
    "            optimizer = optimizer, \n",
    "            data_loader = data_loader,\n",
    "            make_matrix_data_set = make_matrix_data_set,\n",
    "            )\n",
    "\n",
    "        ndcg, hit = evaluate(\n",
    "            model = model, \n",
    "            data_loader = data_loader,\n",
    "            user_train = user_train,\n",
    "            user_valid = user_valid,\n",
    "            make_matrix_data_set = make_matrix_data_set,\n",
    "            )\n",
    "        # print('time:',end_time-start_time)\n",
    "        # if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')\n",
    "            \n",
    "        if hit > best_hit:\n",
    "            best_hit = hit\n",
    "            torch.save(model.state_dict(), config.save_dir + config.saved_file_name)\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= config.patience:\n",
    "                break\n",
    "        if ndcg > best_ndcg:\n",
    "            best_ndcg = ndcg\n",
    "    print(f'Best NDCG@10: {best_ndcg:.5f}| Best HIT@10: {best_hit:.5f}')\n",
    "    \n",
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(config.save_dir + config.saved_file_name))\n",
    "submission = inference(\n",
    "        model = model, \n",
    "        data_loader = data_loader,\n",
    "        decoder = decoder,\n",
    "        make_matrix_data_set = make_matrix_data_set\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>253796</th>\n",
       "      <td>11</td>\n",
       "      <td>733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253797</th>\n",
       "      <td>11</td>\n",
       "      <td>4886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253794</th>\n",
       "      <td>11</td>\n",
       "      <td>8961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253793</th>\n",
       "      <td>11</td>\n",
       "      <td>2115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253792</th>\n",
       "      <td>11</td>\n",
       "      <td>5418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  item\n",
       "253796    11   733\n",
       "253797    11  4886\n",
       "253794    11  8961\n",
       "253793    11  2115\n",
       "253792    11  5418"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"output/submission_cdae.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optuna - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "'data_path' : \"/opt/ml/input/data/train\" , # 데이터 경로\n",
    "'save_dir': './checkpoint/',\n",
    "'saved_file_name':'best_model.pt',\n",
    "'p_dims': [50, 500],\n",
    "'dropout_rate' : 0.7,\n",
    "'weight_decay' : 0.01,\n",
    "'valid_samples' : 10, # 검증에 사용할 sample 수\n",
    "'patience' : 30,\n",
    "'lr' : 0.001,\n",
    "'batch_size' : 500,\n",
    "'num_epochs' : 100,\n",
    "'num_workers' : 2,\n",
    "}\n",
    "config = Box(config)\n",
    "def objective(trial):\n",
    "    config.batch_size = trial.suggest_categorical('batch_size',[256, 512, 1024, 2048])\n",
    "    config.lr = trial.suggest_loguniform('lr',0.001,0.01)\n",
    "    config.weight_decay = trial.suggest_loguniform('weight_decay',1e-07,1e-06)\n",
    "    config.dropout_rate = trial.suggest_categorical(\"dropout_rate\",[0.2,0.3,0.4,0.5,0.6,0.7,0.8])\n",
    "    config.num_layers = trial.suggest_int('num_layers',1 , 4)\n",
    "    config.p_dims = [trial.suggest_int('hidden_dims',50,800)] * config.num_layers\n",
    "    \n",
    "    model = CDAE(\n",
    "    p_dims = config.p_dims + [make_matrix_data_set.num_item],\n",
    "    user_num = make_matrix_data_set.num_user, \n",
    "    item_num = make_matrix_data_set.num_item,\n",
    "    dropout_rate = config.dropout_rate).to(device)\n",
    "    criterion = LossFunc(loss_type = 'Logistic')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    best_hit = -1\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        # start_time = time.time() # 측정 시작 -> 1 epoch 당 5~6초 정도 소요됨\n",
    "        train_loss = train(\n",
    "            model = model, \n",
    "            criterion = criterion, \n",
    "            optimizer = optimizer, \n",
    "            data_loader = data_loader,\n",
    "            make_matrix_data_set = make_matrix_data_set,\n",
    "            )\n",
    "\n",
    "        ndcg, hit = evaluate(\n",
    "            model = model, \n",
    "            data_loader = data_loader,\n",
    "            user_train = user_train,\n",
    "            user_valid = user_valid,\n",
    "            make_matrix_data_set = make_matrix_data_set,\n",
    "            )\n",
    "        if hit > best_hit:\n",
    "            best_hit = hit\n",
    "    return best_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name = 'cdae_parameter_opt',\n",
    "    direction = 'maximize',\n",
    "    sampler = sampler)\n",
    "\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print('best params {} :'.format(fold+1), study.best_value)\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best params {} :',study.best_value)\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "'data_path' : \"/opt/ml/input/data/train\" , # 데이터 경로\n",
    "'save_dir': './checkpoint/',\n",
    "'saved_file_name':'best_model_tanh.pt',\n",
    "'p_dims': [50, 800, 500],\n",
    "'dropout_rate' : 0.7,\n",
    "'weight_decay' : 0.01,\n",
    "'valid_samples' : 10, # 검증에 사용할 sample 수\n",
    "'patience' : 30,\n",
    "'lr' : 0.001,\n",
    "'batch_size' : 500,\n",
    "'num_epochs' : 200,\n",
    "'num_workers' : 2,\n",
    "}\n",
    "for key, value in study.best_params.items():\n",
    "    if key == 'hidden_dims':\n",
    "        config['p_dims'] = [value] * study.best_params['num_layers']\n",
    "    else:\n",
    "        config[key] = value\n",
    "# print(config)\n",
    "\n",
    "config = Box(config)\n",
    "\n",
    "model = CDAE(\n",
    "    p_dims = config.p_dims + [make_matrix_data_set.num_item],\n",
    "    user_num = make_matrix_data_set.num_user, \n",
    "    item_num = make_matrix_data_set.num_item,\n",
    "    dropout_rate = config.dropout_rate).to(device)\n",
    "criterion = LossFunc(loss_type = 'Logistic')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "main(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(config.save_dir + config.saved_file_name))\n",
    "submission = inference(\n",
    "        model = model, \n",
    "        data_loader = data_loader,\n",
    "        decoder = decoder,\n",
    "        make_matrix_data_set = make_matrix_data_set\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"output/submission_cdae_optuna_tanh.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPGnkx0QB11mHCm7DlICatq",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "13Xsup9xhUtDAcsBlzmT4GEkRUr1ZscDK",
   "name": "CDAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
